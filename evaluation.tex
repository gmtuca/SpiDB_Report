\chapter{Evaluation}
\label{cha:eval}
This research project has among its main aims the evaluation of results, described in this chapter. It is important to analyse how this general purpose application performs under the SpiNNaker environments and what architectural limitations it faces, which is useful feedback to the team. The chapter outlines also performance benchmarks of SpiDB in comparison with other database systems and how it can be improved.

\section{Transmission delay \& Latency}
\label{sec:eval_comm_rel}

According to my experiments, sending SDP packets immediately between two cores results on a very large packet drop ration. Without explicit delay between the transmission of packets, the number of successful deliveries is only about 10\% of those sent for a large number of packets. This effect is strongly reduced by the addition of a small delay of 5-10$\mu$s, which guarantees over 99\% successful deliveries, as can be seen on table \ref{table:sdp_deliveries}. The table also shows that having long delays, greater than 10$\mu$s is mostly redundant, as they do not decrease packet loss significantly, but tragically reduce throughput.

Upon experimenting, I found that sending a large amount of consecutive, immediate packets (at least 1,000,000) has a chance of consequently crashing the destination core or causing it to reach the erraneous watchdog state (WDOG), meaning it cannot respond to commands.

\begin{table}
\begin{tabular}{ r | c | c | c | c | c }
 & \multicolumn{5}{c}{\textbf{Successful deliveries (\%)}} \\

\textbf{Packets sent} & \textbf{no delay} & \textbf{2$\mu$s delay} & \textbf{5$\mu$s delay} & \textbf{10$\mu$s delay} & \textbf{100$\mu$s delay} \\
50,000 & 9.56\% & 57.73\% & 95.95\% & 98.36\% & 99.85\% \\
100,000 & 12.15\% & 54.97\% & 97.99\% & 99.13\% & 99.92\% \\
200,000 & 13.07\% & 50.55\% & 99.01\% & 99.33\% & 99.96\% \\
500,000 & 12.97\% & 50.08\% & 99.49\% & 99.80\% & 99.99\% \\
1,000,000 & 13.05\% & 45.06\% & 98.84\% & 99.88\% & 99.99\% \\
\end{tabular}
\caption{Successful SDP deliveries with delay between each packet}
\label{table:sdp_deliveries}
\end{table}

Note that the experiment, made on SpiNN-3, involved sending a single SDP packet multiple times to the same destination on the same chip, containing only the 8 byte SDP header. The destination did not store or read the packet contents, only incrementing a counter upon packet receival. This was used to show the maximum possible transmission rate allowed by the SDP protocol under the hardware communication fabric. Code snippets and more information can be found on the appendix under the appendix section \ref{sec:appendix_comm_rel}.

Ideally these packets would send useful information, to be read upon arrival, which would keep the destination busy. From my experience and input from the team, the best way to achieve this is by immediately storing the SDP packet contents into a buffer when it is received and then handling it at a different point in time (listing \ref{lst:sdp_packet_callback} under appendix). The reason for this is that if another packet arrives as we are processing the current packet with same or higher priority, the incomming packet will drop.

High priority should be assigned to storing incomming packets into a buffer and the actual processing should have lower priority, as it can be handled at a later point in time. It is important to note that this can cause starvation and buffer overflow if there are too many immediate packets being received. For instance, if our SDP packet is of size 12-byte (8-byte header + 32-bit data) and stored into a buffer in private DTCM memory upon receival, we would only ever be able to hold up to about 5,000 messages at once (64-Kbytes DTCM size / 12-byte packet size). Realistically a large part of DTCM contains the stack, local and global variables, so that number will be drastically reduced. In my application, SpiDB, insert and retrieve database queries have a size of 256-bytes, which means a limit of 250 entries in the buffer if memory were empty.

\begin{figure}
\begin{center}
	\includegraphics[width=1.3\textwidth, natwidth=1063, natheight=509]{images/transmission_delay.png}
\end{center}
\caption{PUT performance with variable delay}
\label{fig:transmission-delay}
\end{figure}
	
A measured test, transmitting data from a host machine to an Ethernet attached SpiNNaker chip and then to an Application Processor on that node via shared memory, achieved speeds in excess of 22 Mb/s.\cite{scalablecomm}

This evaluation is important because it allows finding the optimal transmission delay and latency for an application. A developer on SpiNNaker using the SDP protocol, which is experimental and yet to be optimised\cite{scalablecomm}, needs to find the balance between transmission rate and reliability, which are inversely proportional.

On SpiDB, I experimented with different time delays when transmitting packets from host over ethernet to a core on SpiNNaker, in order to find the best evaluation. The speed of one operation is calculated as the time of successful reply minus the time that operation was sent from host, thus being a round-trip-time plus internal processing time. The performance is calculated as the amount of operations successfully executed in one second.

Figure \ref{fig:transmission-delay} plots the performance (left Y axis) and packet drop percentage (right Y axis) of PUT operations  with the decrease of transmission delay. As can be seen, large packet transmission delays of 50-100$\mu$s are redundant, as they do not reduce packet drops, while being a high cost on performance. Naturally the more packets we can send in one second influences the speed of replies, thus improving performance. This hits a maxima at 40$\mu$s, with almost 10,000 op/sec, in which transmission is at a high rate with no loss of reliability. Transmission delays between 40-10$\mu$s result on a decrease of performance, because although packets are sent more frequently, a lot of them are dropped (up to about 35\%), being also extremely unreliable. Naturally performance of is inversely proportional to packet drop rate, as unsuccessful queries do not contribute to the final result. We reach the worst case at 10-5$\mu$s delay, when the destination core cannot cope with the speed of incomming packets, simply crashing and ceasing to respond. A very similar behaviour is visible for \textit{pull} operations, as plotted on figure \ref{fig:hash-performance}.

These SpiDB experiments were performed with 100,000 \textit{put} operations with keys and values of size 4-bytes each. More information on the data gathered for these experiments can be found on the appendix under section \ref{sec:appendix_comm_rel}.

\begin{figure}
\begin{center}
	\includegraphics[width=1.3\textwidth, natwidth=1063, natheight=550]{images/hash_performance.png}
\end{center}
\caption{HASH PULL performance with variable delay}
\label{fig:hash-performance}
\end{figure}

These results are largely influenced by the latency of the SDP datagram, driving the need of such delays. A simulation framework described by Navaridas et al. \cite{impacttraffic} has shown how latency scales with system size by simulating a range of system configurations from 16 × 16 (256 nodes) to 256 × 256 (65,536 nodes). The average and maximum latencies obtained after a simulation of 100,000 cycles are plotted in figure \ref{fig:latency} \cite{scalablecomm}. End-to-end latency scales linearly with the number of nodes per dimension, i.e. $O(\sqrt{N})$ with the number of nodes. The 
maximum latency is that needed to reach the most distant nodes. This means when designing a SpiNNaker application, a developer needs to take into account the locality of communications and the expected latency, which can be used to improve speed of packet transmission.

\begin{figure}
\begin{center}
	\includegraphics[width=.7\textwidth, natwidth=336, natheight=243]{images/latency.png}
\end{center}
\caption{Latency scalability under uniform and local traffic distributions}
\medskip
\footnotesize
Local distribution emulates close proximity destinations. Traffic following a uniform distribution represents a very pessimistic scenario in which locality is not exploited.
\label{fig:latency}
\end{figure}

%\includegraphics[width=1\textwidth, natwidth=1063, natheight=509]{images/naive_performance.png}

%what are reasons for packets to drop? because thingy just takes 1 microsecond or does it?...

\section{Performance benchmark}
In order to evaluate the performance of my database management system, I compared it with two of the most popular RAM-based Key-value pair databases: \textbf{memcached}\cite{memcached} and \textbf{Redis}\cite{redis}. These systems are extremely powerful and have been on the market for multiple years, thus serve as good basis to push forward my application, SpiDB.

Figure \ref{fig:write-perf-benchmark} shows a performance plot of the SpiNNaker Database (SpiDB) against these competitors for 100,000 consecutive \textit{put} operations. It can be seen that SpiDB runs at about 7,000-8,000 operations per second, which is slower than the others by a factor of 8. Performance for all systems was mostly constant given input size, up to the maximum SDP data size of 256-bytes. More specification and data gathered can be found at appendix under section \ref{sec:appendix-specs}.

\begin{figure}
\begin{center}
	\includegraphics[width=1.4\textwidth, natwidth=1063, natheight=509]{images/write_performance.png}
\end{center}
\caption{PUT operation performance benchmark}
\label{fig:write-perf-benchmark}
\end{figure}

The current SpiDB implementation is new and can be highly enhanced with modifications to the network communication and the addition of caching, which increases the overall performance, enhancing the application in comparison to the other systems. Nevertheless there is a limit to how much improvement can be brought using the SDP protocol, as it has a network and memory bottleneck, earlier discussed in sections \ref{sec:eval_comm_rel} and \ref{sec:limitations}. This means improvements to the code would never allow it to reach performances above around 10,000 operations per second, which is still very slow in comparison with the others.

Although from an initial evaluation it may seem that SpiNNaker is not capable of hosting a database efficiently, that can be proven wrong. It must be outlined that this performance cap is the limitation of a single, independent SpiNNaker board. The SpiNNaker hardware was designed to be inter-connected, allowing transfers of data between boards, which can be used to add another degree of scalability. This means multiple boards can be connected to the host/user and process a set of separate queries in a fully parallel way. Ideally this would result in a linear performance increase proportional to the number of boards connected, meaning that two boards will run the database operations twice faster than one board alone and so forth. From this analysis I can state that an instance of SpiDB running on 8-10 different 4-chip boards will perform better than \textit{Redis} or \textit{memcached} on commercial hardware. The SpiNNaker hardware is expensive to produce for now, but with a decrease on its prices in the future, it can be a very strong candidate for hosting a DBMS.

This scalability work has not yet been fully tested, thus it may serve as constructive speculation for now. It should also be noted that realistically a perfectly linear performance increase with the increase of horsepower is not common, as other factors need to be taken into account, such as synchronization, fault tolerance, etc.

%response times. why are they so high sometimes?!?!

\section{SDRAM bandwidth utilisation}
\label{sec:limitations}
this figure shows the aggregate SDRAM bandwidth utilization of a number of cors on a single SpiNNaker CMP, increased progressively from 1 to 14. DMA access to memory.

It is clear from the figure that the read channel saturates just over 600 MB/s and the write channel adds around 300 MB/s on top of that, for a total aggregate bandwidth of 900 MB/s. Because memory reads and writes share the same network DMA channel, an increase in the number of writes decreases the read bandwidth. DMA bandwidth, although saturated, is shared fairly among all cores. 

The progression of Core 6 on the plot shows that if the DMA controller is free, a single core can read from shared memory at 300 MB/s, but if such memory is being constantly accessed other cores, each will only receive data at a rate of around 50 MB/s (6 times slower than doing so exclusively).

From these results I can evaluate that, although SDRAM is useful to store and share large amounts of data, it suffers when being accessed frequently accessed by multiple cores, which is exactly the case of the SpiDB \textit{pull} or \textit{SELECT} queries.


From this evaluation, looking at the progression of Core 6 in the figure

\cite{painkras}


\begin{figure}
\begin{center}
	\includegraphics[width=1.3\textwidth, natwidth=820, natheight=329]{images/sdram_bandwidth.png}
\end{center}
\caption{SDRAM bandwidth utilization}
\label{fig:sdram_bandwidth}
\end{figure}

\section{Future work}
This project has been the tip of the iceberg of what a fully-functional SpiNNaker database management system can be, involving mostly research and simple operations to evaluate its performance. All code written by me is now part of the official open-source SpiNNaker API, available to the team and any developers interested. This means SpiDB is likely to expand in the future or serve as an example application running on SpiNNaker, accessible to researchers around the globe.

These are some features which can be implemented or improved in the future:

\begin{itemize}
	\item \textbf{Caching} and \textbf{indexing}: frequently accessed areas of shared SDRAM memory could be cached at the smaller but much faster private DTCM.
	\item \textbf{Security} and \textbf{multi-user access}: different sections of the database can have restricted access through credentials checking.
	\item \textbf{Scalability testing} on the large scale million core machine.
	\item An application \textbf{server} allowing queries to be requested over the internet on different locations.
	\item Improve \textbf{reliability} and increase \textbf{query sizes}, perhaps by implementing a protocol on top of the \textit{SDP} and \textit{MC} layer. As of now packets are limited to 256-bytes, with unreliability during busy times.
	\item \textbf{Self balancing} during idle times. While no queries are being executed, cores could distribute their contents in a balanced way for faster retrival. Indexing or other pre-processing could also be executed on the meantime.
	\item \textbf{Additional operations} supporting table merges, triggers and aggregations.
\end{itemize}